---
title: AI 专业术语表
date: 2025-04-26 18:27:11
---

### agent（智能体）​
一种以大语言模型驱动的人工智能程序，能够自主感知环境并采取行动以实现目标，拥有自主推理决策、规划行动、检索记忆、选择工具执行任务等能力。​

### AI hallucination（AI 幻觉）​
AI 生成的内容与现实世界的知识不一致或与实际数据显著不同的现象。​

### application program interface（API，应用程序接口）​
应用程序交互所需的一组定义和协议。API 描述了程序必须使用的方法和数据格式，以与其他软件进行通信。比如，OpenAI 允许开发人员通过 API 使用 GPT-4 和 ChatGPT。​

### artificial intelligence（AI，人工智能）​
计算机科学的一个领域，专注于创建算法以执行传统上由人类执行的任务，比如处理自然语言、分析图像、解决复杂问题和做出决策。​

### artificial neural network（人工神经网络）​
大语言模型在生成信息时可以处理的目标标记周围的文本范围。上下文窗口大小对于理解和生成与特定上下文相关的文本至关重要。一般而言，较大的上下文窗口可以提供更丰富的语义信息。​

### deep learning（DL，深度学习）​
机器学习的一个子领域，专注于训练具有多层的神经网络，从而实现对复杂模式的学习。​

### embedding（嵌入）​
表示词语或句子且能被机器学习模型处理的实值向量。对于值较为接近的向量，它们所表示的词语或句子也具有相似的含义。在信息检索等任务中，嵌入的这种特性特别有用。​

### Facebook AI Similarity Search（Faiss，Facebook AI 相似性搜索）​
Facebook AI 团队开源的针对聚类和相似性搜索的库，为稠密向量提供高效的相似性搜索和聚类，支持十亿级别向量的搜索，是目前较为成熟的近似近邻搜索库。​

### few-shot learning（少样本学习）​
一种仅用很少的示例训练机器学习模型的技术。对于大语言模型而言，这种技术可以根据少量的输入示例和输出示例来引导模型响应。​

### fine-tuning（微调）​
在微调过程中，预训练模型（如 GPT-3 或其他大语言模型）在一个较小、特定的数据集上进一步训练。微调旨在重复使用预训练模型的特征，并使其适应于特定任务。对于神经网络来说，这意味着保持结构不变，仅稍微改变模型的权重，而不是从头开始构建模型。​

### foundation model（基础模型）​
一类 AI 模型，包括但不限于大语言模型。基础模型是在大量未标记数据上进行训练的。这类模型可以执行各种任务，如图像分析和文本翻译。基础模型的关键特点是能够通过无监督学习从原始数据中学习，并能够通过微调来执行特定任务。​

### function call（函数调用）​
OpenAI 开发的一项功能，它允许开发人员在调用 GPT 模型的 API 时，描述函数并让模型智能地输出一个包含调用这些函数所需参数的 JSON 对象。利用它，我们可以更可靠地将 GPT 的能力与外部工具和 API 相结合。​

### Generative AI（GenAI，生成式人工智能）​
人工智能的一个子领域，专注于通过学习现有数据模式或示例来生成新的内容，包括文本、代码、图像、音频等，常见应用包括聊天机器人、创意图像生成和编辑、代码辅助编写等。​

### Generative Pre-trained Transformer（GPT，生成式预训练 Transformer）​
由 OpenAI 开发的一种大语言模型。GPT 基于 Transformer 架构，并在大量文本数据的基础上进行训练。这类模型能够通过迭代地预测序列中的下一个单词来生成连贯且与上下文相关的句子。​

### inference（推理）​
使用训练过的机器学习模型进行预测和判断的过程。​

### information retrieval（信息检索）​
在一组资源中查找与给定查询相关的信息。信息检索能力体现了大语言模型从数据集中提取相关信息以回答问题的能力。​
LangChain​
一个 Python 软件开发框架，用于方便地将大语言模型集成到应用程序中。​

### language model（语言模型）​
用于自然语言处理的人工智能模型，能够阅读和生成人类语言。语言模型是对词序列的概率分布，通过训练文本数据来学习一门语言的模式和结构。​
large language model（LLM，大语言模型）​
具有大量参数（参数量通常为数十亿，甚至千亿以上）的语言模型，经过大规模文本语料库的训练。GPT-4 和 ChatGPT 就属于 LLM，它们能够生成自然语言文本、处理复杂语境并解答难题。​

### long short-term memory（LSTM，长短期记忆）​
一种用于处理序列数据中的短期及长期依赖关系的循环神经网络架构。然而，基于 Transformer 的大语言模型（如 GPT 模型）不再使用 LSTM，而使用注意力机制。​

### machine learning（ML，机器学习）​
人工智能的一个子领域，其主要任务是创建智能算法。这些算法就像学生一样，它们从给定的数据中自主学习，无须人类逐步指导。​

### machine translation（机器翻译）​
使用自然语言处理和机器学习等领域的概念，结合 Seq2Seq 模型和大语言模型等模型，将文本从一门语言翻译成另一门语言。​

### multimodal model（多模态模型）​
能够处理和融合多种数据的模型。这些数据可以包括文本、图像、音频、视频等不同模态的数据。它为计算机提供更接近于人类感知的场景。​

### n-gram​
一种算法，常用于根据词频预测字符串中的下一个单词。这是一种在早期自然语言处理中常用的文本补全算法。后来，n-gram 被循环神经网络取代，再后来又被基于 Transformer 的算法取代。​

### natural language processing（NLP，自然语言处理）​
人工智能的一个子领域，专注于计算机与人类之间的文本交互。它使计算机程序能够处理自然语言并做出有意义的回应。​

### OpenAI​
位于美国的一个人工智能实验室，它由非营利实体和营利实体组成。OpenAI 是 GPT 等模型的开发者。这些模型极大地推动了自然语言处理领域的发展。​
### OpenAPI​
OpenAPI 规范是描述 HTTP API 的标准，它允许消费者与远程服务进行交互，而无须提供额外的文档或访问源代码。OpenAPI 规范以前被称为 Swagger 规范。​

### parameter（参数）​
对大语言模型而言，参数是它的权重。在训练阶段，模型根据模型创建者选择的优化策略来优化这些系数。参数量是模型大小和复杂性的衡量标准。参数量经常用于比较大语言模型。一般而言，模型的参数越多，它的学习能力和处理复杂数据的能力就越强。​

### plugin（插件）​
一种专门为语言模型设计的独立封装软件模块，用于扩展或增强模型的能力，可以帮助模型检索外部数据、执行计算任务、使用第三方服务等。​

### pre-trained（预训练）​
机器学习模型在大型和通用的数据集上进行的初始训练阶段。对于一个新给定的任务，预训练模型可以针对该任务进行微调。​

### prompt（提示词）​
输入给语言模型的内容，模型通过它生成一个输出。比如，在 GPT 模型中，提示词可以是半句话或一个问题，模型将基于此补全文本。​

### prompt engineering（提示工程）​
设计和优化提示词，以从语言模型中获得所需的输出。这可能涉及指定响应的格式，在提示词中提供示例，或要求模型逐步思考。​

### prompt injection（提示词注入）​
一种特定类型的攻击，通过在提示词中提供精心选择的奖励，使大语言模型的行为偏离其原始任务。​

### recurrent neural network（RNN，循环神经网络）​
一类表现出时间动态行为的神经网络，适用于涉及序列数据的任务，如文本或时间序列。​

### reinforcement learning（RL，强化学习）​
一种机器学习方法，专注于在环境中训练模型以最大化奖励信号。模型接收反馈并利用该反馈来进一步学习和自我改进。​

### reinforcement learning from human feedback（RLHF，通过人类反馈进行强化学习）​
一种将强化学习与人类反馈相结合的训练人工智能系统的先进技术，该技术涉及使用人类反馈来创建奖励信号，继而使用该信号通过强化学习来改进模型的行为。​

### sequence-to-sequence model（Seq2Seq 模型，序列到序列模型）​
这类模型将一个领域的序列转换为另一个领域的序列。它通常用于机器翻译和文本摘要等任务。Seq2Seq 模型通常使用循环神经网络或 Transformer 来处理输入序列和输出序列。​

### supervised fine-tuning（SFT，监督微调）​
采用预先训练好的神经网络模型，并针对特定任务或领域在少量的监督数据上对其进行重新训练。​

### supervised learning（监督学习）​
一种机器学习方法，可以从训练资料中学到或建立一个模式，以达到准确分类或预测结果的目的。​

### synthetic data（合成数据）​
人工创建的数据，而不是从真实事件中收集的数据。当真实数据不可用或不足时，我们通常在机器学习任务中使用合成数据。比如，像 GPT 这样的语言模型可以为各种应用场景生成文本类型的合成数据。​

### temperature（温度）​
大语言模型的一个参数，用于控制模型输出的随机性。温度值越高，模型结果的随机性越强；温度值为 0 表示模型结果具有确定性（在 OpenAI 模型中，温度值为 0 表示模型结果近似确定）。​

### text completion（文本补全）​
大语言模型根据初始的单词、句子或段落生成文本的能力。文本是根据下一个最有可能出现的单词生成的。​

### token（标记）​
字母、字母对、单词或特殊字符。在自然语言处理中，文本被分解成标记。在大语言模型分析输入提示词之前，输入提示词被分解成标记，但输出文本也是逐个标记生成的。​

### tokenization（标记化）​
将文本中的句子、段落切分成一个一个的标记，保证每个标记拥有相对完整和独立的语义，以供后续任务使用（比如作为嵌入或者模型的输入）。​

### transfer learning（迁移学习）​
一种机器学习技术，其中在一个任务上训练的模型被重复利用于另一个相关任务。比如，GPT 在大量文本语料库上进行预训练，然后可以使用较少的数据进行微调，以适用于特定任务。​

### Transformer architecture（Transformer 架构）​
一种常用于自然语言处理任务的神经网络架构。它基于自注意力机制，无须顺序处理数据，其并行性和效率高于循环神经网络和长短期记忆模型。GPT 基于 Transformer 架构。​

### unsupervised learning（无监督学习）​
一种机器学习方法，它使用机器学习算法来分析未标记的数据集并进行聚类。这些算法无须人工干预即可发现隐藏的模式或给数据分组。​

### zero-shot learning（零样本学习）​
一个机器学习概念，即大语言模型对在训练期间没有明确见过的情况进行预测。任务直接呈现在提示词中，模型利用其预训练的知识生成回应。