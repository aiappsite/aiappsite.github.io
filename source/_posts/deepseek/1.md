---
title: 初识 DeepSeek
tags: [DeepSeek, LLM]
---

[DeepSeek 入门](https://scc.ustc.edu.cn/_upload/article/files/bd/11/edc7c00b4726b6f09c82d41cb3d5/7fed3cfc-7ff4-40cd-8762-e5e62913d6b8.pdf)

2025 年春节，中国 AI 领域迎来里程碑式突破 —— 深度求索（DeepSeek）推出的 671B 参数大模型，以 1/30 的训练成本实现了与 GPT-4o 相当的推理能力。这一成果不仅打破了国际巨头的技术垄断，更通过开源战略重塑了全球 AI 生态。


| 模型        |	参数量 |	训练成本（H800 GPU 小时） |	推理延迟（ms/token）|	中文准确率 |
|----        | --    | --                         | --                | -- |
|DeepSeek-V3 |	671B |	2,788k                   |	80	               |92.3%|
|GPT-4o	     | 1.8T	 |18,500k	                 |150                |85.6%|
|Claude3.5	 | 70B	 |3,200k                     |120	              |78.9%|

## DeepSeek 基本概念
![](https://cdn.jsdelivr.net/gh/0326/imgs@main/blog/20250426163926352.png)
打开官网你会发现有深度思考R1，联网搜索选项，这些都有什么作用呢？这里先对相关术语解释一下：
- **DeepSeek**: 泛指 DeepSeek 系列模型；
- **DeepSeek-V3**: deepseek基座模型（无深度思考能力），其指令版本具备对话能力，与gpt-4o，qwen2.5系列等模型属于同阶段模型。
- **DeepSeek-R1**: 推理模型，擅长复杂问题的推理，准确率相较于 DeepSeek V3 更高，但思考过程过长。
- **DeepSeek-R1-zero**: 推理模型，可以理解deepseek R1的先验版本，R1-zero的训练是一个探索性的过程，它验证了RL本身对于激励base模型产生推理的能力。在这个探索结论上，才开始正式进入R1的训练。
- **DeepSeek-R1-Distill-Qwen-xxxB**: 知识蒸馏版的推理模型，使用deepseek R1中间阶段的训练数据，对Qwen2.5系列进行SFT指令微调的模型。（无强化学习过程，可以理解为COT思维链数据的SFT）大家平时听到的残血版，蒸馏版，大多指此版本。
- **联网搜索**: 大模型知识来源于海量的离线数据训练，其训练数据大约滞后于其发布时间半年到一年以上，开启联网搜索可以保证知识的及时性。


## DeepSeek 的核心技术
### 混合专家（MoE）架构
传统 AI 模型像一个全能型天才，包揽所有任务但效率低下。DeepSeek 则像一家高效的公司：
专家团队：内部有多个 “小专家” 模块，分别擅长不同任务（如数学计算、语言翻译、代码生成）。
智能调度：当用户提问时，系统会自动判断需要哪些专家协作。例如，回答 “如何用 Python 计算斐波那契数列” 时，会调用 “编程专家” 和 “数学专家”。
节省资源：每次只激活相关专家，避免浪费算力。例如，671B 参数的模型实际仅使用 37B 参数处理任务，速度提升 3 倍以上。

### 稀疏注意力
传统 AI 处理长文本时，会逐字分析所有内容，耗时且占用大量资源。DeepSeek 则像人类阅读时的 “选择性关注”：
动态分层策略：
全局视角：快速扫描整体内容，抓住核心主题（如书籍的章节标题）。
局部聚焦：深入分析关键段落（如公式推导部分）。
跳跃式阅读：跳过无关信息（如广告或重复内容）。
硬件优化：通过特殊设计的 “计算高速公路”，让信息传递路径最短，处理 64k 长度的文本时速度提升 11.6 倍。

### 多模态能力
DeepSeek 能同时理解文字、图像、声音等多种信息，类似人类的 “五感融合”：
跨模态对齐：将图片、语音等转化为统一的 “数字语言”，例如将 “苹果” 的文字描述与图片特征对应。
多模态生成：结合文字和图像生成内容，例如根据 “夕阳下的海滩” 文字描述生成对应图片，或为图片添加生动的文字说明。


## 训练与优化：让 AI 快速成长的 “秘密武器”
### 数据处理：海量知识的 “精挑细选”
DeepSeek 的 “知识库” 包含数万亿条数据，但并非所有内容都有用：
清洗过滤：去除重复、错误或低质量的信息（如广告、乱码）。
动态增强：对数据进行 “变形” 处理，例如将代码中的变量重命名、打乱句子顺序，让模型学会识别本质规律。

### 分布式训练：全球协作的 “超级工厂”
训练 DeepSeek 需要处理海量数据，传统方法耗时且成本高。DeepSeek 采用 “三维并行策略”：
数据并行：将数据分成小块，同时交给多个 GPU 处理（类似工厂流水线）。
模型并行：将模型拆分成多个部分，分布到不同 GPU 上（如 “数学模块” 在 GPU1，“语言模块” 在 GPU2）。
流水线并行：按步骤处理数据，前一步骤的输出作为后一步骤的输入（类似汽车组装线）。

### 推理优化：让 AI “秒级响应” 的魔法
为了让 DeepSeek 快速回答问题，工程师做了以下优化：
模型量化：将模型参数 “压缩” 到更小的存储空间，例如用 4 位数字表示原来的 32 位数字，节省 75% 内存。
混合精度计算：在保证准确性的前提下，使用更高效的计算方式（如 FP8 格式），速度提升 2 倍。
注意力核优化：针对特定硬件（如 NVIDIA H800 GPU）优化计算流程，减少冗余操作。

# 总结
DeepSeek 通过混合专家架构、稀疏注意力、多阶段训练等核心技术，实现了大模型性能与成本的革命性突破。其开源战略和硬件协同优化，为开发者提供了高效的 AI 基础设施。未来，随着量子计算、自进化系统等技术的融合，DeepSeek 有望进一步推动通用人工智能的发展。